{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dewasahu/pdf-extractor?scriptVersionId=143433409\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-18T15:55:07.420721Z","iopub.execute_input":"2023-09-18T15:55:07.421087Z","iopub.status.idle":"2023-09-18T15:55:07.495645Z","shell.execute_reply.started":"2023-09-18T15:55:07.421057Z","shell.execute_reply":"2023-09-18T15:55:07.494644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#install all packages\n!pip install tokenizer transformers scikit-learn pypdf==3.16.0 nltk pandarallel pandas==2.1.0 datasets #pyspark pyarrow","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-09-18T15:55:07.4978Z","iopub.execute_input":"2023-09-18T15:55:07.498529Z","iopub.status.idle":"2023-09-18T15:55:19.933826Z","shell.execute_reply.started":"2023-09-18T15:55:07.498496Z","shell.execute_reply":"2023-09-18T15:55:19.932645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all imports\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport datasets\nimport transformers","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:55:19.935663Z","iopub.execute_input":"2023-09-18T15:55:19.936056Z","iopub.status.idle":"2023-09-18T15:55:19.944988Z","shell.execute_reply.started":"2023-09-18T15:55:19.936017Z","shell.execute_reply":"2023-09-18T15:55:19.944046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#resume_data=pd.read_csv(\"kaggle resume dataset filepath\") change it accordingly\nresume_data=pd.read_csv(\"/kaggle/input/resume-dataset/Resume/Resume.csv\")\nresume_data=resume_data.drop([\"Resume_html\"],axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:55:19.946787Z","iopub.execute_input":"2023-09-18T15:55:19.94728Z","iopub.status.idle":"2023-09-18T15:55:20.659989Z","shell.execute_reply.started":"2023-09-18T15:55:19.947249Z","shell.execute_reply":"2023-09-18T15:55:20.65887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting all text from a pdf\nfrom pypdf import PdfReader\n\ndef pdf_text(filePath:str)->str:\n    reader = PdfReader(filePath)\n    text=\"\"\n    for page in reader.pages:\n        text+=page.extract_text()\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:55:20.663625Z","iopub.execute_input":"2023-09-18T15:55:20.664114Z","iopub.status.idle":"2023-09-18T15:55:20.66978Z","shell.execute_reply.started":"2023-09-18T15:55:20.664078Z","shell.execute_reply":"2023-09-18T15:55:20.668753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#doing text cleaninig and tokenization\nfrom nltk import pos_tag\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nfrom nltk.corpus import stopwords\nimport string\nimport re\n\npuncuation=set(string.punctuation)\nstop_words_english=set(stopwords.words(\"english\"))\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    sentences = sent_tokenize(text)\n    features = {'feature': \"\"}\n\n    for sent in sentences:\n        for criteria in ['skills', 'education']:\n            if criteria in sent:\n                words = word_tokenize(sent)\n                words = [word for word in words if word not in stop_words_english]\n                # POS tagger to identify and remove stop words and other irrelevant words\n                tagged_words = pos_tag(words)\n                filtered_words = [word for word, tag in tagged_words if tag not in ['DT', 'IN', 'TO', 'PRP', 'WP']]\n                features['feature'] += \" \".join(filtered_words)\n\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:55:20.671249Z","iopub.execute_input":"2023-09-18T15:55:20.671852Z","iopub.status.idle":"2023-09-18T15:55:20.683047Z","shell.execute_reply.started":"2023-09-18T15:55:20.671817Z","shell.execute_reply":"2023-09-18T15:55:20.682016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"progress_bar=tqdm(range(len(resume_data)))\n# extract text from csv and get the text from pdf\ndef process(df):\n    #uncomment when using pdf_text function .. done to save time\n    id=df['ID']\n    category=df['Category'] \n    text=pdf_text(f\"/kaggle/input/resume-dataset/data/data/{category}/{id}.pdf\")\n   # text=str(df[\"Resume_str\"])\n    features=preprocess_text(text)\n    df['Feature']=features['feature']\n    progress_bar.update(1)\n    return df\n      \n#applying processing to resume_data\nresume_data=resume_data.apply(process,axis=1)\nresume_data=resume_data.drop(columns=['Resume_str'])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T15:55:20.684734Z","iopub.execute_input":"2023-09-18T15:55:20.685377Z","iopub.status.idle":"2023-09-18T16:17:30.899791Z","shell.execute_reply.started":"2023-09-18T15:55:20.685341Z","shell.execute_reply":"2023-09-18T16:17:30.898659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to avoid computing again and save time\n#resume_data.to_csv(\"/path_to_save_processed_resume_data\")\nresume_data.to_csv(\"/kaggle/working/resume_data.csv\",index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#resume_data=pd.read_csv(\"/path_to_save_processed_resume_data\")\nresume_data=pd.read_csv(\"/kaggle/working/resume_data.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:17:30.901272Z","iopub.execute_input":"2023-09-18T16:17:30.901672Z","iopub.status.idle":"2023-09-18T16:17:31.113866Z","shell.execute_reply.started":"2023-09-18T16:17:30.901636Z","shell.execute_reply":"2023-09-18T16:17:31.112864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resume_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:17:31.115274Z","iopub.execute_input":"2023-09-18T16:17:31.115665Z","iopub.status.idle":"2023-09-18T16:17:31.126367Z","shell.execute_reply.started":"2023-09-18T16:17:31.11563Z","shell.execute_reply":"2023-09-18T16:17:31.125347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resume_data['Category'].value_counts().sort_index().plot(kind=\"bar\",figsize=(12,6))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:17:31.128276Z","iopub.execute_input":"2023-09-18T16:17:31.128971Z","iopub.status.idle":"2023-09-18T16:17:31.581423Z","shell.execute_reply.started":"2023-09-18T16:17:31.128936Z","shell.execute_reply":"2023-09-18T16:17:31.580543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting job description\nnum_desc=15 #number of jod_description to fetch from job_description_data\n\n#job_description=pd.read_csv(\"job_description_data.csv\")\njob_description=pd.read_csv(\"/kaggle/input/resume-and-job-description/training_data.csv\")\n\n#choosing 15 job description\njob_description=job_description[[\"job_description\",\"position_title\"]][:num_desc]","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:17:31.582903Z","iopub.execute_input":"2023-09-18T16:17:31.583498Z","iopub.status.idle":"2023-09-18T16:17:32.047978Z","shell.execute_reply.started":"2023-09-18T16:17:31.583451Z","shell.execute_reply":"2023-09-18T16:17:32.046919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# processing the text of the job description\njob_description['Features']=job_description['job_description'].apply(lambda x : preprocess_text(x)['feature'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#choosing 15 job description\njob_description=job_description[[\"job_description\",\"position_title\"]][:num_desc]\n# processing the text of the job description\njob_description['Features']=job_description['job_description'].apply(lambda x : preprocess_text(x)['feature'])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:24:00.497266Z","iopub.execute_input":"2023-09-18T16:24:00.497854Z","iopub.status.idle":"2023-09-18T16:24:00.930481Z","shell.execute_reply.started":"2023-09-18T16:24:00.497822Z","shell.execute_reply":"2023-09-18T16:24:00.929394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"job_description.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:24:08.313714Z","iopub.execute_input":"2023-09-18T16:24:08.314297Z","iopub.status.idle":"2023-09-18T16:24:08.325613Z","shell.execute_reply.started":"2023-09-18T16:24:08.314259Z","shell.execute_reply":"2023-09-18T16:24:08.324397Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using pytorch along with ðŸ¤—transformers...\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\ndevice=\"cuda\"if torch.cuda.is_available() else \"cpu\"\n\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n\n#putting model and tokenizer to gpu for faster processing \nmodel.to(device)\n\n\ndef get_embeddings(text):\n    inputs = tokenizer(str(text), return_tensors=\"pt\",truncation=True,padding=True).to(device)\n    outputs = model(**inputs)\n    #getting the embedding to cpu\n    embeddings = outputs.last_hidden_state.mean(dim=1).detach().to(\"cpu\").numpy() \n    return embeddings\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:17:32.500481Z","iopub.execute_input":"2023-09-18T16:17:32.501886Z","iopub.status.idle":"2023-09-18T16:17:34.484393Z","shell.execute_reply.started":"2023-09-18T16:17:32.501858Z","shell.execute_reply":"2023-09-18T16:17:34.483395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nimport pandas as pd\n\n\n# Calculate embeddings for all job descriptions and resumes\njob_desc_embeddings = np.array([get_embeddings(desc) for desc in job_description['Features']])\nresume_embeddings = np.array([get_embeddings(text) for text in resume_data['Feature']])\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:17:34.485887Z","iopub.execute_input":"2023-09-18T16:17:34.486463Z","iopub.status.idle":"2023-09-18T16:18:44.406368Z","shell.execute_reply.started":"2023-09-18T16:17:34.486426Z","shell.execute_reply":"2023-09-18T16:18:44.405195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sqeezing job embeding \njob_desc_embeddings=job_desc_embeddings.squeeze()\nresume_embeddings=resume_embeddings.squeeze()\nresume_embeddings.shape,job_desc_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:18:44.408194Z","iopub.execute_input":"2023-09-18T16:18:44.40855Z","iopub.status.idle":"2023-09-18T16:18:44.415746Z","shell.execute_reply.started":"2023-09-18T16:18:44.408516Z","shell.execute_reply":"2023-09-18T16:18:44.414807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize a DataFrame to store the results\nresult_df = pd.DataFrame(columns=['jobId', 'resumeId', 'similarity', 'domainResume', 'domainDesc'])\n\n# top k-resumes\nk=5\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:18:44.417203Z","iopub.execute_input":"2023-09-18T16:18:44.417789Z","iopub.status.idle":"2023-09-18T16:18:44.427152Z","shell.execute_reply.started":"2023-09-18T16:18:44.417757Z","shell.execute_reply":"2023-09-18T16:18:44.426208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Iterate over job descriptions\nfor i, job_desc_emb in enumerate(job_desc_embeddings):\n    job_desc_id = i\n    job_title = job_description['position_title'].iloc[i]\n\n    # Compute cosine similarities between the current job description and all resumes\n    similarities = cosine_similarity([job_desc_emb], resume_embeddings )\n\n    # Get the indices of the top-k most similar resumes\n    top_k_indices = np.argsort(similarities[0])[::-1][:k]\n   \n    # Extract the relevant information and add it to the result DataFrame\n    for j in top_k_indices:\n        resume_id = resume_data['ID'].iloc[j]\n        work_domain = resume_data['Category'].iloc[j]\n        similarity_score = similarities[0][j]\n        \n        result_df.loc[i+j] = [job_desc_id, resume_id, similarity_score, work_domain,job_title ]\n        \n\n# Sort the results by similarity score (descending)\nresult_df = result_df.sort_values(by='similarity', ascending=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:18:44.428726Z","iopub.execute_input":"2023-09-18T16:18:44.429095Z","iopub.status.idle":"2023-09-18T16:18:44.735196Z","shell.execute_reply.started":"2023-09-18T16:18:44.429064Z","shell.execute_reply":"2023-09-18T16:18:44.7339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:18:44.739327Z","iopub.execute_input":"2023-09-18T16:18:44.740435Z","iopub.status.idle":"2023-09-18T16:18:44.758461Z","shell.execute_reply.started":"2023-09-18T16:18:44.740377Z","shell.execute_reply":"2023-09-18T16:18:44.757281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_group=result_df.groupby(\"jobId\")\nresult_group","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:18:44.759986Z","iopub.execute_input":"2023-09-18T16:18:44.760738Z","iopub.status.idle":"2023-09-18T16:18:44.76873Z","shell.execute_reply.started":"2023-09-18T16:18:44.760698Z","shell.execute_reply":"2023-09-18T16:18:44.767832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#example of group data\nresult_group.get_group(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:18:44.77015Z","iopub.execute_input":"2023-09-18T16:18:44.77124Z","iopub.status.idle":"2023-09-18T16:18:44.789378Z","shell.execute_reply.started":"2023-09-18T16:18:44.771208Z","shell.execute_reply":"2023-09-18T16:18:44.78834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(num_desc):\n    print()\n    print(\"jobId---cosineSimilarity---domainResume---domainDesc\")\n    print(result_group.get_group(i).values[0])\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T16:18:44.790787Z","iopub.execute_input":"2023-09-18T16:18:44.791404Z","iopub.status.idle":"2023-09-18T16:18:44.809951Z","shell.execute_reply.started":"2023-09-18T16:18:44.791372Z","shell.execute_reply":"2023-09-18T16:18:44.809Z"},"trusted":true},"execution_count":null,"outputs":[]}]}